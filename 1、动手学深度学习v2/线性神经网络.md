### 一、线性回归

* 线性模型可以看作一层的神经网络
* 线性回归是对n维输入的加权，外加偏差
* 线性回归使用平方损失来衡量预测值和真实值的差异，损失函数是凸函数，有显式解(精确解)

### 二、基础优化算法

##### **1、学习率和Batch_size**：

学习率不能太大也不能太小，学习率太小会导致收敛很慢，消耗计算资源；学习率太大可能会导致跳过收敛点，也不合适。

由于计算梯度比较昂贵，直接对所有数据进行损失梯度下降计算非常昂贵，从所有数据中选择一部分计算损失，以代表总体的损失，这一部分数据的规模叫做批量大小，批量大小就是代替总体的样本大小。

在选取批量大小上，不能太小，太小不适合并行无法最大利用计算资源；也不适合太大，太大内存消耗增加浪费资源。

总结：

* 梯度下降通过不断沿着反梯度方向更新参数求解
* 小批量随即梯度下降是深度学习默认的求解算法
* 两个重要的超参数是批量大小和学习率

##### 2、深刻认识随机梯度下降算法

通过类比牛顿法和随机梯度下降(SGD: stochastic gradient descent ) 来**深刻认识神经网络和随机梯度下降算法**。

在数学理论上牛顿法可能更优，但在神经网络中进行参数调优仍通常使用随机梯度下降（SGD: stochastic gradient descent）法。

其原理是牛顿法利用到损失函数的二阶导数(Hessian矩阵)信息，直接跳到二次近似最小值。而随机梯度下降法沿着一阶导数下降最快的方向以合适的学习率不断迭代寻找使得损失函数最小的参数最优解。因此从理论上牛顿法要快于随机梯度下降(stochastic gradient descent)。但在具体实践过程中，几乎不使用牛顿法，以下是几点原因：

* 牛顿法寻找最优解最适合损失函数是凸函数的情况，但实践中损失函数可能异常复杂，极少存在凸函数的损失函数(这是因为神经网络常常处理NP-complete问题，无法找到精确解)，牛顿法在非凸函数中很可能收敛在鞍点，而非极值点。
* 在具体实践过程中，数据量很可能异常庞大，我们知道损失函数是标量，其一阶导数是向量，二阶导是矩阵。牛顿法要用到Hessian矩阵和Hessian矩阵的逆矩阵，需要求二阶导和二阶导的逆，对内存的消耗达到O(n^2)甚至O（n^3）的程度，这对大规模数据量训练是不可接受的。

使用随机梯度下降法的原因：

* 随机梯度下降法使用迭代收敛的方式，每次只取一个小批次(Batch)，内存消耗少，非常适合大数据集。
* 使用小批次(Batch)进行参数调优，而非计算二阶Hessian矩阵，有利于充分利用GPU的并行计算功能，提高资源利用率。
* 随机梯度下降中利用到随机，有助于跳出局部最优解。
* 随机梯度下降中的随机其实给数据带来的噪声，在高纬度深层次的神经网络训练中，有利于提高模型对噪声的抗干扰能力，提高模型的鲁棒性和泛化性能，避免过拟合。

### 三、softmax回归

softmax回归其实是一个分类问题。回归的输出是一个符合自然区间的数值，损失函数是预测值和真实值的差距。分类通常有多个输出，第i个输出是预测为第i类的置信度。

在 **Softmax回归**（多类逻辑回归）中，使用 **交叉熵损失（Cross-Entropy Loss）** 而非 **均方误差（MSE）** 作为损失函数，主要基于以下理论和实践原因：

交叉熵的本质：



