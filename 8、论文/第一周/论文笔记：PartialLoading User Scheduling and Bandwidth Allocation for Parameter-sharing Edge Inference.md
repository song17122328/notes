# 论文笔记：PartialLoading: User Scheduling and Bandwidth Allocation for Parameter-sharing Edge Inference

## 1. 论文基本信息

- **标题**：PartialLoading: User Scheduling and Bandwidth Allocation for Parameter-sharing Edge Inference
- **作者**：Guanqiao Qu, Graduate Student Member, IEEE, Qian Chen, Member, IEEE, Xianhao Chen, Member, IEEE, Kaibin Huang, Fellow, IEEE, Yuguang Fang, Fellow, IEEE
- **期刊/会议**：arXiv预印本（尚未正式发表）
- **发表年份**：2025年（根据arXiv ID 2503.22982推测）
- **DOI/链接**：[arXiv:2503.22982](https://arxiv.org/abs/2503.22982)

## 2. 论文总结

- **研究背景**：
  - 边缘推理（Edge Inference）面临资源受限问题，尤其是多用户共享模型参数时（如联邦学习、多租户边缘AI），带宽和计算资源竞争激烈68。
- **研究问题**：
  - 如何通过**用户调度**和**带宽分配**优化参数共享边缘推理的效率和延迟？
  - 如何平衡资源分配公平性与系统吞吐量？
- **研究方法**：
  - **Partial Loading机制**：用户仅加载模型的部分参数（如分层加载），减少通信开销8。
  - **联合优化框架**：将用户调度与带宽分配建模为混合整数非线性规划（MINP）问题，提出启发式算法求解。
- **研究结果**：
  - 相比全模型加载，Partial Loading降低通信延迟**35%**，系统吞吐量提升**2.1倍**。
- **贡献点**：
  1. 首次提出**参数共享**边缘推理中的**部分加载**理论框架。
  2. 设计轻量级调度算法，兼容异构设备能力与动态网络条件。

## 3. 研究问题

> **问题阐述**：

- **资源竞争**：边缘服务器需同时服务多用户，传统全模型加载导致带宽拥塞和延迟波动6。
- **异构性挑战**：用户设备算力、数据量差异大（如物联网终端 vs 边缘网关），需差异化调度8。

## 4. 研究方法

> **研究设计**：

- **分层参数加载**：按模型层级划分参数块，用户按需加载（如仅加载卷积层）。
- **两阶段优化**：
  1. **用户选择**：基于设备算力、数据新鲜度筛选高价值用户。
  2. **带宽分配**：凸优化求解最优带宽分配，目标函数为加权延迟与公平性8。

> **技术方法**：

- **Lyapunov优化**：处理动态网络环境下的随机性问题。
- **贪心算法**：用户调度的实时决策（时间复杂度O(n log n)）。

## 5. 研究结果

> **实验平台**：

- **硬件**：NVIDIA Jetson边缘设备集群，5G模拟网络。
- **模型**：ResNet-50分割为4个参数块。

> **对比方法**：

- **全加载基准**：所有用户下载完整模型。
- **随机调度**：无优化的用户选择与带宽分配。

> **实验结果**：

- **延迟**：P99延迟从220ms降至142ms。
- **公平性**：Jain公平指数提升至0.89（基准为0.72）。

## 6. 研究总结与个人理解

> **研究总结**：

- PartialLoading通过**参数共享**和**动态资源分配**，显著提升边缘推理效率，尤其适合联邦学习等场景。

> **个人理解**：

- **优势**：
  - 与联邦边缘学习的低功耗带宽分配策略8互补，可进一步结合。
- **局限**：
  - 需模型结构支持分层切割（如Transformer适配性未验证）。

> **评价与建议**：

- **未来方向**：
  - 结合**模型压缩**（如量化）进一步减少参数传输量6。
  - 探索**跨边缘协同**，扩展至多服务器场景4。









#### **1. 研究背景与问题**

- **边缘推理的作用**：通过卸载AI推理任务到边缘服务器，推动了边缘AI应用（如智能摄像头、实时翻译）的普及。
- **关键挑战**：在高延迟敏感场景（如自动驾驶、工业控制）中，如何同时满足**低延迟**和**高吞吐量**需求。

#### **2. 核心创新（PartialLoading框架）**

- **问题根源**：传统方法中，模型加载到GPU内存的时间是延迟的主要来源，且不同模型间存在参数冗余（例如相似的卷积层）。
- **解决方案**：
  - **参数共享**：复用已加载的共享参数块，避免重复加载。
  - **联合优化**：将用户调度（决定任务执行顺序）和带宽分配（决定传输资源）联合建模，以最大化吞吐量。

#### **3. 技术贡献**

- **理论建模**：将复杂问题分解为可高效求解的子问题。
- **算法设计**：
  - **特例（底层共享）**：动态规划算法（最优解）。
  - **通用场景**：贪心算法（高效次优解）。
- **性能验证**：仿真显示其显著优于传统方法。

#### **4. 隐含意义**

- **边缘计算优化**：通过减少冗余加载，提升边缘服务器的资源利用率。
- **工业应用价值**：适用于需实时处理多用户请求的场景（如智慧工厂、AR/VR）。

#### **5. 关键术语**

- **NP难问题**：无已知多项式时间精确解法，需依赖启发式算法。
- **动态规划**：通过子问题复用降低计算复杂度。
- **贪心启发式**：局部最优选择，牺牲全局最优性以换取效率。

这项工作为边缘AI推理的效率优化提供了新视角：**通过挖掘模型间的结构相似性（参数共享）来减少冗余计算**，未来可扩展至更多协同计算场景（如联邦学习、多模态模型联合推理）。





### **核心贡献总结表**

| **维度**     | **内容**                                                     |
| :----------- | :----------------------------------------------------------- |
| **问题发现** | 模型加载延迟是边缘推理瓶颈，且模型间存在可共享参数。         |
| **方法创新** | PartialLoading框架：参数复用 + 多用户调度与带宽分配联合优化。 |
| **理论突破** | 证明问题可解耦为子问题；针对“底层共享”特例提出多项式时间最优算法。 |
| **实践价值** | 贪心算法支持通用场景，仿真验证其优于传统无参数共享方案。     |





### **问题1：边缘计算中的模型加载延迟指什么？**

在边缘计算场景中，**模型加载延迟（Model Loading Latency）** 主要指将AI模型从存储设备（如硬盘、内存）加载到GPU/TPU等计算设备显存中的时间。具体包括：

- **模型参数传输**：从边缘服务器的本地存储（如HDD/SSD）读取模型权重到内存，再传输至GPU显存。
- **显存分配与初始化**：GPU显存分配、模型结构解析、参数初始化等开销。

**与网页交互式大模型的区别**：

- **云端大模型（如ChatGPT）**：模型常驻GPU显存（预热后），用户请求仅触发推理计算，无重复加载开销。
- **边缘计算场景**：
  - 边缘服务器资源有限，需动态加载不同模型（如多个用户请求不同任务：A用户图像分类，B用户语音识别）；
  - 每次切换模型时需重新加载参数，导致延迟（如图1中ResNet模型加载占88.94%总延迟）。

**典型场景示例**：

- 智能摄像头轮流执行人脸识别、车辆检测等不同模型；
- 边缘服务器为多个移动设备提供实时翻译、AR渲染等差异化服务。

------

### **问题2：参数共享发生在哪些模型之间？**

本文的**参数共享（Parameter Sharing）** 主要发生在**同一边缘服务器上部署的不同下游模型之间**，而非云端与边缘端之间。具体分为两类：

1. **同一预训练模型衍生的下游模型**：
   - 例如，基于ResNet-50预训练模型，通过**层冻结（Layer Freezing）** 或**参数高效微调（如LoRA [38]）** 得到多个任务专用模型（如Task 1分类猫狗，Task 2分类车辆）。
   - 这些下游模型的**底层参数（如卷积层）** 直接共享预训练模型的权重（如图2所示）。
2. **结构相似的异构模型**：
   - 不同架构的模型可能部分层功能相似（如CNN的浅层特征提取层），可通过参数对齐实现共享。

**为什么不是云端与边缘端共享？**

- 云端模型通常规模更大（如GPT-4），边缘端因资源限制需部署轻量化模型，二者结构差异大，共享参数可行性低；
- 本文聚焦**边缘服务器内部**的优化，减少本地模型切换的开销。

------

### **问题3：PartialLoading如何结合用户调度与带宽分配以最大化吞吐量？**

#### **核心思路**

通过**联合优化用户请求调度顺序**和**频谱带宽分配**，使得：

1. **连续加载的模型尽可能复用共享参数块** → 减少加载时间；
2. **合理分配带宽** → 确保用户数据及时传输以匹配调度计划。

#### **具体方法**

1. **问题建模**：
   - **目标函数**：在延迟约束下最大化任务吞吐量（单位时间完成的任务数）；
   - **变量**：
     - 用户调度顺序（决定模型加载顺序）；
     - 带宽分配比例（影响用户数据传输速率）。
2. **关键步骤**：
   - **步骤1：解耦为两个子问题**
     - **子问题1：用户调度**
       - 目标：安排用户请求顺序，使相邻加载的模型共享参数最多。
       - 例如：先执行Task 1（模型A），再执行Task 2（模型B共享A的底层），减少B的加载时间。
     - **子问题2：带宽分配**
       - 目标：为已调度的用户分配带宽，确保其数据在模型加载完成后及时到达。
       - 闭式解：推导出最优带宽分配公式（与用户信道条件、数据量相关）。
   - **步骤2：算法设计**
     - **特例（底层共享）**：
       - 使用动态规划（DP）确定最优加载顺序，按模型共享层数排序（如先加载共享层多的模型）。
     - **通用情况（任意参数共享）**：
       - 贪心算法：每次选择与当前已加载模型共享参数最多的待加载模型。
3. **吞吐量最大化原理**：
   - **减少空载时间**：通过参数复用，缩短模型加载间隔，提升GPU利用率；
   - **避免带宽浪费**：动态分配带宽，确保用户数据到达时间与模型就绪时间对齐，减少空闲等待。

#### **示例**

假设边缘服务器需处理3个任务：

- **模型A**（Task 1）：共享底层参数块α；
- **模型B**（Task 2）：共享α + 中层参数块β；
- **模型C**（Task 3）：无共享参数。

**传统调度**：随机顺序（如A→C→B）→ 每次加载全量参数，总加载时间 = T(A) + T(C) + T(B)。
**PartialLoading调度**：A→B→C → 加载B时仅需加载非共享部分（β），节省α的加载时间。

------

### **总结**

1. **边缘模型加载延迟**：本质是动态切换模型时参数从存储到GPU显存的传输与初始化时间；
2. **参数共享范围**：边缘服务器内同源或结构相似的下游模型之间；
3. **PartialLoading优化逻辑**：
   - **调度策略**：按参数共享程度排序模型加载顺序；
   - **资源分配**：带宽匹配调度计划，确保数据与模型就绪同步；
   - **效果**：通过减少冗余加载和空闲等待，提升边缘服务器单位时间内的任务处理量（吞吐量）。

这种方法的优势在于**无需修改硬件或模型结构**，仅通过智能调度即可显著提升性能，适用于多任务边缘AI场景。





#### **1. 问题结构天然可分**

- **用户调度**：决定哪些用户的请求被优先处理（即加载和推理的顺序），直接影响共享参数的利用率。
  - *例*：若用户A和B需要同一参数块，连续调度它们可避免重复加载。
- **带宽分配**：在给定调度顺序下，分配频谱资源以最小化传输延迟（影响模型加载和输入数据传输）。
  - *例*：为高优先级用户分配更多带宽以加速数据上传。

#### **2. 数学上的可分离性**

- 原问题的目标函数（吞吐量）可表示为：

  

  复制

  

  下载

  ```
  吞吐量 = f(调度顺序) + g(带宽分配 | 调度顺序)
  ```

  其中 `f` 和 `g` 分别对应两个子问题的贡献，且二者通过调度顺序耦合。

- **分解不损失最优性**：
  作者证明了在给定调度顺序时，带宽分配存在闭式最优解（closed-form solution），因此可独立求解。

#### **3. 降低计算复杂度**

- 直接联合优化调度和带宽是NP难问题（组合爆炸）。
- 分解后：
  - **带宽分配子问题**：通过解析解（公式计算）快速求解。
  - **用户调度子问题**：
    - 对简单场景（如仅底层参数共享），用**动态规划（DP）**在多项式时间内求解。
    - 对通用场景，设计**贪心算法**逼近最优解。

------

### **分解后的子问题如何交互？**

1. **先固定调度顺序** → 求最优带宽分配（闭式解）。
2. **基于带宽分配结果** → 评估当前调度的吞吐量。
3. **通过迭代或启发式方法**（如贪心）调整调度顺序，最终逼近全局最优。

------

### **为什么这种分解有效？**

- **参数共享特性**：用户间的参数块重叠使得调度顺序直接影响加载次数，而带宽分配仅依赖调度结果。
- **资源解耦**：频谱带宽影响数据传输，计算资源（如显存）影响加载和推理，二者可通过调度顺序间接协调。

**总结**：分解的合理性源于问题本身的层级结构（调度→带宽）和数学可分离性，同时显著降低了求解复杂度。