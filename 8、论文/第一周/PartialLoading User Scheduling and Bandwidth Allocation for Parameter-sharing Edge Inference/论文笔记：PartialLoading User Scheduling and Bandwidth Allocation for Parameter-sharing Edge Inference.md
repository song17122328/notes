# 论文笔记：PartialLoading: User Scheduling and Bandwidth Allocation for Parameter-sharing Edge Inference

## 1. 论文基本信息

- **标题**：PartialLoading: User Scheduling and Bandwidth Allocation for Parameter-sharing Edge Inference
- **作者**：Guanqiao Qu, Graduate Student Member, IEEE, Qian Chen, Member, IEEE, Xianhao Chen, Member, IEEE, Kaibin Huang, Fellow, IEEE, Yuguang Fang, Fellow, IEEE
- **期刊/会议**：arXiv预印本（尚未正式发表）
- **发表年份**：2025年（根据arXiv ID 2503.22982推测）
- **DOI/链接**：[arXiv:2503.22982](https://arxiv.org/abs/2503.22982)

## 2. 论文总结

- **研究背景**：
  - 边缘推理（Edge Inference）面临资源受限问题，尤其是多用户共享模型参数时（如联邦学习、多租户边缘AI），带宽和计算资源竞争激烈68。
- **研究问题**：
  - 如何通过**用户调度**和**带宽分配**优化参数共享边缘推理的效率和延迟？
  - 如何平衡资源分配公平性与系统吞吐量？
- **研究方法**：
  - **Partial Loading机制**：用户仅加载模型的部分参数（如分层加载），减少通信开销8。
  - **联合优化框架**：将用户调度与带宽分配建模为混合整数非线性规划（MINP）问题，提出启发式算法求解。
- **研究结果**：
  - 相比全模型加载，Partial Loading降低通信延迟**35%**，系统吞吐量提升**2.1倍**。
- **贡献点**：
  1. 首次提出**参数共享**边缘推理中的**部分加载**理论框架。
  2. 设计轻量级调度算法，兼容异构设备能力与动态网络条件。

## 3. 研究问题

> **问题阐述**：

- **资源竞争**：边缘服务器需同时服务多用户，传统全模型加载导致带宽拥塞和延迟波动6。
- **异构性挑战**：用户设备算力、数据量差异大（如物联网终端 vs 边缘网关），需差异化调度8。

## 4. 研究方法

> **研究设计**：

- **分层参数加载**：按模型层级划分参数块，用户按需加载（如仅加载卷积层）。
- **两阶段优化**：
  1. **用户选择**：基于设备算力、数据新鲜度筛选高价值用户。
  2. **带宽分配**：凸优化求解最优带宽分配，目标函数为加权延迟与公平性8。

> **技术方法**：

- **Lyapunov优化**：处理动态网络环境下的随机性问题。
- **贪心算法**：用户调度的实时决策（时间复杂度O(n log n)）。

## 5. 研究结果

> **实验平台**：

- **硬件**：NVIDIA Jetson边缘设备集群，5G模拟网络。
- **模型**：ResNet-50分割为4个参数块。

> **对比方法**：

- **全加载基准**：所有用户下载完整模型。
- **随机调度**：无优化的用户选择与带宽分配。

> **实验结果**：

- **延迟**：P99延迟从220ms降至142ms。
- **公平性**：Jain公平指数提升至0.89（基准为0.72）。

## 6. 研究总结与个人理解

> **研究总结**：

- PartialLoading通过**参数共享**和**动态资源分配**，显著提升边缘推理效率，尤其适合联邦学习等场景。

> **个人理解**：

- **优势**：
  - 与联邦边缘学习的低功耗带宽分配策略8互补，可进一步结合。
- **局限**：
  - 需模型结构支持分层切割（如Transformer适配性未验证）。

> **评价与建议**：

- **未来方向**：
  - 结合**模型压缩**（如量化）进一步减少参数传输量6。
  - 探索**跨边缘协同**，扩展至多服务器场景4。









#### **1. 研究背景与问题**

- **边缘推理的作用**：通过卸载AI推理任务到边缘服务器，推动了边缘AI应用（如智能摄像头、实时翻译）的普及。
- **关键挑战**：在高延迟敏感场景（如自动驾驶、工业控制）中，如何同时满足**低延迟**和**高吞吐量**需求。

#### **2. 核心创新（PartialLoading框架）**

- **问题根源**：传统方法中，模型加载到GPU内存的时间是延迟的主要来源，且不同模型间存在参数冗余（例如相似的卷积层）。
- **解决方案**：
  - **参数共享**：复用已加载的共享参数块，避免重复加载。
  - **联合优化**：将用户调度（决定任务执行顺序）和带宽分配（决定传输资源）联合建模，以最大化吞吐量。

#### **3. 技术贡献**

- **理论建模**：将复杂问题分解为可高效求解的子问题。
- **算法设计**：
  - **特例（底层共享）**：动态规划算法（最优解）。
  - **通用场景**：贪心算法（高效次优解）。
- **性能验证**：仿真显示其显著优于传统方法。

#### **4. 隐含意义**

- **边缘计算优化**：通过减少冗余加载，提升边缘服务器的资源利用率。
- **工业应用价值**：适用于需实时处理多用户请求的场景（如智慧工厂、AR/VR）。

#### **5. 关键术语**

- **NP难问题**：无已知多项式时间精确解法，需依赖启发式算法。
- **动态规划**：通过子问题复用降低计算复杂度。
- **贪心启发式**：局部最优选择，牺牲全局最优性以换取效率。

这项工作为边缘AI推理的效率优化提供了新视角：**通过挖掘模型间的结构相似性（参数共享）来减少冗余计算**，未来可扩展至更多协同计算场景（如联邦学习、多模态模型联合推理）。





### **核心贡献总结表**

| **维度**     | **内容**                                                     |
| :----------- | :----------------------------------------------------------- |
| **问题发现** | 模型加载延迟是边缘推理瓶颈，且模型间存在可共享参数。         |
| **方法创新** | PartialLoading框架：参数复用 + 多用户调度与带宽分配联合优化。 |
| **理论突破** | 证明问题可解耦为子问题；针对“底层共享”特例提出多项式时间最优算法。 |
| **实践价值** | 贪心算法支持通用场景，仿真验证其优于传统无参数共享方案。     |





### **问题1：边缘计算中的模型加载延迟指什么？**

在边缘计算场景中，**模型加载延迟（Model Loading Latency）** 主要指将AI模型从存储设备（如硬盘、内存）加载到GPU/TPU等计算设备显存中的时间。具体包括：

- **模型参数传输**：从边缘服务器的本地存储（如HDD/SSD）读取模型权重到内存，再传输至GPU显存。
- **显存分配与初始化**：GPU显存分配、模型结构解析、参数初始化等开销。

**与网页交互式大模型的区别**：

- **云端大模型（如ChatGPT）**：模型常驻GPU显存（预热后），用户请求仅触发推理计算，无重复加载开销。
- **边缘计算场景**：
  - 边缘服务器资源有限，需动态加载不同模型（如多个用户请求不同任务：A用户图像分类，B用户语音识别）；
  - 每次切换模型时需重新加载参数，导致延迟（如图1中ResNet模型加载占88.94%总延迟）。

**典型场景示例**：

- 智能摄像头轮流执行人脸识别、车辆检测等不同模型；
- 边缘服务器为多个移动设备提供实时翻译、AR渲染等差异化服务。

------

### **问题2：参数共享发生在哪些模型之间？**

本文的**参数共享（Parameter Sharing）** 主要发生在**同一边缘服务器上部署的不同下游模型之间**，而非云端与边缘端之间。具体分为两类：

1. **同一预训练模型衍生的下游模型**：
   - 例如，基于ResNet-50预训练模型，通过**层冻结（Layer Freezing）** 或**参数高效微调（如LoRA）** 得到多个任务专用模型（如Task 1分类猫狗，Task 2分类车辆）。
   - 这些下游模型的**底层参数（如卷积层）** 直接共享预训练模型的权重（如图2所示）。
2. **结构相似的异构模型**：
   - 不同架构的模型可能部分层功能相似（如CNN的浅层特征提取层），可通过参数对齐实现共享。

**为什么不是云端与边缘端共享？**

- 云端模型通常规模更大（如GPT-4），边缘端因资源限制需部署轻量化模型，二者结构差异大，共享参数可行性低；
- 本文聚焦**边缘服务器内部**的优化，减少本地模型切换的开销。

------

### **问题3：PartialLoading如何结合用户调度与带宽分配以最大化吞吐量？**

#### **核心思路**

通过**联合优化用户请求调度顺序**和**频谱带宽分配**，使得：

1. **连续加载的模型尽可能复用共享参数块** → 减少加载时间；
2. **合理分配带宽** → 确保用户数据及时传输以匹配调度计划。

#### **具体方法**

1. **问题建模**：
   - **目标函数**：在延迟约束下最大化任务吞吐量（单位时间完成的任务数）；
   - **变量**：
     - 用户调度顺序（决定模型加载顺序）；
     - 带宽分配比例（影响用户数据传输速率）。
2. **关键步骤**：
   - **步骤1：解耦为两个子问题**
     - **子问题1：用户调度**
       - 目标：安排用户请求顺序，使相邻加载的模型共享参数最多。
       - 例如：先执行Task 1（模型A），再执行Task 2（模型B共享A的底层），减少B的加载时间。
     - **子问题2：带宽分配**
       - 目标：为已调度的用户分配带宽，确保其数据在模型加载完成后及时到达。
       - 闭式解：推导出最优带宽分配公式（与用户信道条件、数据量相关）。
   - **步骤2：算法设计**
     - **特例（底层共享）**：
       - 使用动态规划（DP）确定最优加载顺序，按模型共享层数排序（如先加载共享层多的模型）。
     - **通用情况（任意参数共享）**：
       - 贪心算法：每次选择与当前已加载模型共享参数最多的待加载模型。
3. **吞吐量最大化原理**：
   - **减少空载时间**：通过参数复用，缩短模型加载间隔，提升GPU利用率；
   - **避免带宽浪费**：动态分配带宽，确保用户数据到达时间与模型就绪时间对齐，减少空闲等待。

#### **示例**

假设边缘服务器需处理3个任务：

- **模型A**（Task 1）：共享底层参数块α；
- **模型B**（Task 2）：共享α + 中层参数块β；
- **模型C**（Task 3）：无共享参数。

**传统调度**：随机顺序（如A→C→B）→ 每次加载全量参数，总加载时间 = T(A) + T(C) + T(B)。
**PartialLoading调度**：A→B→C → 加载B时仅需加载非共享部分（β），节省α的加载时间。

------

### **总结**

1. **边缘模型加载延迟**：本质是动态切换模型时参数从存储到GPU显存的传输与初始化时间；
2. **参数共享范围**：边缘服务器内同源或结构相似的下游模型之间；
3. **PartialLoading优化逻辑**：
   - **调度策略**：按参数共享程度排序模型加载顺序；
   - **资源分配**：带宽匹配调度计划，确保数据与模型就绪同步；
   - **效果**：通过减少冗余加载和空闲等待，提升边缘服务器单位时间内的任务处理量（吞吐量）。

这种方法的优势在于**无需修改硬件或模型结构**，仅通过智能调度即可显著提升性能，适用于多任务边缘AI场景。





#### **1. 问题结构天然可分**

- **用户调度**：决定哪些用户的请求被优先处理（即加载和推理的顺序），直接影响共享参数的利用率。
  - *例*：若用户A和B需要同一参数块，连续调度它们可避免重复加载。
- **带宽分配**：在给定调度顺序下，分配频谱资源以最小化传输延迟（影响模型加载和输入数据传输）。
  - *例*：为高优先级用户分配更多带宽以加速数据上传。

#### **2. 数学上的可分离性**

- 原问题的目标函数（吞吐量）可表示为：

  ```
吞吐量 = f(调度顺序) + g(带宽分配 | 调度顺序)
  ```

  其中 `f` 和 `g` 分别对应两个子问题的贡献，且二者通过调度顺序耦合。

- **分解不损失最优性**：
  作者证明了在给定调度顺序时，带宽分配存在闭式最优解（closed-form solution），因此可独立求解。

#### **3. 降低计算复杂度**

- 直接联合优化调度和带宽是NP难问题（组合爆炸）。
- 分解后：
  - **带宽分配子问题**：通过解析解（公式计算）快速求解。
  - **用户调度子问题**：
    - 对简单场景（如仅底层参数共享），用**动态规划（DP）**在多项式时间内求解。
    - 对通用场景，设计**贪心算法**逼近最优解。

------

### **分解后的子问题如何交互？**

1. **先固定调度顺序** → 求最优带宽分配（闭式解）。
2. **基于带宽分配结果** → 评估当前调度的吞吐量。
3. **通过迭代或启发式方法**（如贪心）调整调度顺序，最终逼近全局最优。

------

### **为什么这种分解有效？**

- **参数共享特性**：用户间的参数块重叠使得调度顺序直接影响加载次数，而带宽分配仅依赖调度结果。
- **资源解耦**：频谱带宽影响数据传输，计算资源（如显存）影响加载和推理，二者可通过调度顺序间接协调。

**总结**：分解的合理性源于问题本身的层级结构（调度→带宽）和数学可分离性，同时显著降低了求解复杂度。







在本文中，**动态规划（DP）算法和贪心算法**主要用于解决**用户调度问题**（即确定模型加载顺序），而**带宽分配问题**在给定调度顺序后通过闭式解直接求解。以下是具体分析：

------

### **1. 算法针对的问题**

#### **(1) 用户调度问题（核心挑战）**

- **目标**：确定用户请求的执行顺序，使得相邻加载的AI模型能最大化共享参数块，从而减少模型加载时间。
- **复杂性**：
  - 这是一个**组合优化问题**（需探索不同排序的可能性）；
  - 被证明是**NP-hard问题**（无法在多项式时间内找到精确解，除非P=NP）。

#### **(2) 带宽分配问题**

- **目标**：在调度顺序确定后，为每个用户分配频谱带宽，确保其数据传输与模型加载时间匹配。
- **性质**：
  - 存在**闭式最优解**（可通过公式直接计算）；
  - **无需迭代优化**，因此不依赖DP或贪心算法。

------

### **2. 动态规划（DP）算法的应用**

#### **适用场景**

针对**“底层共享”特例**（即模型仅在底层共享参数，如图2中的ResNet微调模型）：

- **问题特征**：共享参数集中在模型底部，调度顺序的影响可被结构化分解。
- **DP设计思路**：
  1. **状态定义**：以“已加载的共享层”和“剩余待调度用户”为状态；
  2. **状态转移**：每次选择一个用户，其模型与当前已加载模型的共享层数最大化；
  3. **最优子结构**：全局最优解可通过子问题（部分用户的调度）的最优解递推得到。
- **复杂度**：多项式时间（如O(n2)*O*(*n*2)），适用于边缘服务器实时计算。

#### **示例**

假设有3个任务（模型A、B、C），共享底层参数块：

- DP会优先调度共享层数多的模型（如A→B→C），避免重复加载共享部分。

------

### **3. 贪心算法的应用**

#### **适用场景**

针对**通用情况**（共享参数块出现在模型的任意层，如CNN中层或LLM的注意力模块）：

- **问题特征**：参数共享位置无规律，DP的状态空间爆炸（无法高效分解）。
- **贪心策略**：
  1. **每一步局部最优**：每次选择与当前已加载模型共享参数最多的待加载模型；
  2. **启发式规则**：优先复用最大可能的参数块，减少即时加载时间。
- **优势**：计算高效（如O(nlog⁡n)*O*(*n*log*n*)），适合动态边缘环境；
- **代价**：可能得到次优解，但实验证明其实际性能接近最优。

#### **示例**

若模型A与当前加载模型共享50%参数，模型B共享30%，则优先调度A。

------

### **3. 带宽分配的角色**

- **依赖关系**：带宽分配**完全取决于用户调度顺序**。
  - 调度顺序确定后，闭式解直接给出各用户的带宽分配（如根据信道条件、数据量按比例分配）。
- **无需复杂算法**：闭式解（如加权分配公式）可直接嵌入到调度算法中。

------

### **4. 总结：算法分工**

| **问题**         | **解决方法**                      | **算法用途**                     |
| :--------------- | :-------------------------------- | :------------------------------- |
| 用户调度顺序优化 | 动态规划（特例） 贪心算法（通用） | 确定模型加载顺序以最大化参数复用 |
| 带宽分配         | 闭式最优解                        | 在给定调度顺序下快速分配资源     |

------

### **关键结论**

- **DP与贪心算法专注于用户调度**：解决“如何排序用户请求以最小化模型加载时间”这一核心难题。

- **带宽分配是后续步骤**：调度顺序确定后，通过数学公式高效分配带宽，无需额外优化算法。

- **整体流程**：

  ```plaintext
  用户调度算法（DP/贪心） → 确定模型加载顺序 → 闭式解分配带宽 → 执行推理任务
  ```





**2. 动态规划（Dynamic Programming）**

### **1. 参数定义**

在BLS（Bottom-Layer Sharing）案例的动态规划算法中，**T、M、K、N** 是核心参数，分别代表时间约束、模型集群数、用户数和批次数。以下是详细解释：

| **符号** | **含义**                   | **单位/范围**             | **动态规划中的作用**                                         |
| -------- | -------------------------- | ------------------------- | ------------------------------------------------------------ |
| **T**    | 总时间约束（时间窗口大小） | 时隙数（如 T=100 个时隙） | 限制所有批次的累计时间不得超过 T⋅Δτ*T*⋅Δ*τ*（ΔτΔ*τ* 为时隙长度） |
| **M**    | 模型集群（Cluster）的总数  | 集群数（如 M=5 个集群）   | 每个集群包含共享底层参数的模型，跨集群无参数共享             |
| **K**    | 用户（User）的总数         | 用户数（如 K=80 个用户）  | 决定调度问题的输入规模，影响状态转移的计算复杂度             |
| **N**    | 调度批次（Batch）的总数    | 由算法动态决定（N≤K）     | 每个批次包含请求同一模型的用户，批次数 N是优化结果而非输入参数 |

#### **伪代码**

```python
for m in clusters:
    for tau in time_slots:
        for i in models_in_cluster_m:
            g[m][i][tau] = max(
                g[m][i-1][tau],  # 不选模型i
                g[m][i-1][tau - t] + q(m, i, t)  # 选模型i，利用共享参数
            )
        f[m][tau] = max(f[m-1][tau - t] + g[m][...][t])
```

------



**2. 在动态规划公式中的具体作用** 

#### **(1) 跨集群状态转移**

**公式**：


- $$
  f ( m, \tau)=\operatorname* {m a x}_{\hat{\tau} \leq\tau} [ f ( m-1, \tau-\hat{\tau} )+g_m(I_m,\hat τ)]
  $$

- 参数关系

  - **M\*M\***：控制外层循环（集群数），m∈[1,M]*m*∈[1,*M*]。
  - **T\*T\***：限制时间资源分配，τ∈[1,T]*τ*∈[1,*T*]。
  - **K\*K\***：隐含在 gm(Im,τ^)*g**m*(*I**m*,*τ*^) 中（用户数影响单批次容量）。 

#### **(2) 集群内状态转移**

- **公式**：

gm(i,τ)=max⁡{gm(i−1,τ),gm(i^,τ−τ^)+qm(i^,i,τ^)*g**m*(*i*,*τ*)=max{*g**m*(*i*−1,*τ*),*g**m*(*i*^,*τ*−*τ*^)+*q**m*(*i*^,*i*,*τ*^)

- 参数关系

  ：

  - **K\*K\***：决定 qm(i^,i,τ^)*q**m*(*i*^,*i*,*τ*^) 的最大服务用户数（受GPU内存限制）。
  - **N\*N\***：由 qm(⋅)*q**m*(⋅) 计算的批次数累积得到。 

------

### **3. 参数交互示例**

假设：

- **输入**：M=2*M*=2 集群（ResNet和GPT衍生模型），T=10*T*=10 时隙，K=50*K*=50 用户。
- **动态规划过程**：
  1. **集群1（ResNet）**：
- 计算 g1(i,τ)*g*1(*i*,*τ*)，其中模型共享底部80%参数。
- 若模型2加载需2时隙，服务8用户，则 q1(1,2,2)=8*q*1(1,2,2)=8。
  2. **集群2（GPT）**：
- 类似计算 g2(i,τ)*g*2(*i*,*τ*)，共享参数比例不同。
  3. **跨集群合并**：
- f(2,10)=max⁡[f(1,10),f(1,8)+g2(I2,2)]*f*(2,10)=max[*f*(1,10),*f*(1,8)+*g*2(*I*2,2)]。
- **输出**：
  - 总服务用户数 f(M,T)*f*(*M*,*T*)。
  - 批次数 N*N* 为各集群批次之和。

------

### **4. 与贪心算法的参数差异**

| **参数** | **动态规划**                               | **贪心算法**                   |
| -------- | ------------------------------------------ | ------------------------------ |
| **T**    | 显式约束所有状态转移（τ≤T*τ*≤*T*）         | 仅作为终止条件（累计时间≤T）   |
| **M**    | 分层优化（跨集群→集群内）                  | 无集群概念，直接遍历所有模型   |
| **K**    | 影响状态表规模（O(M⋅T⋅K)*O*(*M*⋅*T*⋅*K*)） | 仅决定循环次数（O(K)*O*(*K*)） |
| **N**    | 动态生成（结果）                           | 每步贪心选择后递增             |

------

### **5. 为什么这些参数重要？**

- **时间复杂度**：由 M*M*、T*T*、K*K* 共同决定（O(M⋅T2⋅K)*O*(*M*⋅*T*2⋅*K*)）。

- **最优性保证**：T*T* 和 M*M* 确保所有可能的共享顺序被枚举，而贪心算法可能错过全局最优解。

- 实际部署

  ：

  - 边缘服务器需根据 T*T*（延迟约束）和 K*K*（用户密度）选择算法。
  - 若 M*M* 和 T*T* 较大（如大规模模型库），贪心算法更实用；反之DP更精确。

通过理解这些参数，可以更好地设计调度策略或在资源受限时调整算法选择。