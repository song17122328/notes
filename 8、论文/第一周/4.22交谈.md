前两篇论文针对LLM推理优化：

**DistServe**将模型推理的预填充与解码分离，将预填充应用于独立的GPU组(A100)，解码应用于独立的GPU组（H100）

预填充属于计算密集型，可以使用张量并行(模型并行的子类)，加快计算

解码(IO密集型)使用流水线并行策略。

预填充完成后生成的KV Cache直接通过NVlink转移到解码GPU组。

**Orac**将云端模型推理过程中的请求批处理拆分成迭代批处理。将原来的将一批请求放到GPU里面并行计算转变化将一个请求拆分为不同的迭代(迭代可以理解为生成一个token，也即一个解码过程)。通过配置并行处理不同请求的不同迭代过程，避免了短请求等长请求的现象，减少了系统延迟，增加了模型推理的并行程度。

后两篇论文：PartialLoading和Hermes聚焦在边缘设备的模型加载和模型推理上。

