摘要：请你帮我生成

一、知识学习

​	本周主要完成了李沐动手深度学习3章的学习：包括计算性能、计算机视觉、循环神经网络。距离整本书学完还剩一章注意力机制，下周可以完成这本书的学习。

​	计算性能章节主要讨论了不同并行策略的特点的使用场景，如数据并行、模型并行(包括层间并行和算子并行)、流水线并行。而后介绍了基于数据并行的单机多卡训练和分布式GPU集群训练，学习了相关的概念和流程。

​	计算机视觉介绍了视觉领域的经典任务和对应的处理方法及模型的简要实现，包括图像增广技术、迁移学习(微调技术，PartialLoading论文讨论问题的基础)、目标检测任务、语义分割和风格迁移。

​	通过循环神经网络这一章节开始了NLP的学习，与视觉领域不同，NLP领域的深度神经网络都是自回归的序列模型，这一章节可以为后面理解和掌握LLM技术打下基础。

​	通过本周的学习，我基本对神经网络这门课程有了较为清晰的认识，对相关论文设计到的技术背景也有了基本的了解，下一步在学习其他知识的同时还要重视对深度学习的复习巩固。

二、论文学习、

**PartialLoading: User Scheduling and Bandwidth Allocation for Parameter-sharing Edge Inference**

基于参数共享边缘推理的用户调度和带宽分配

三个问题了解这篇文章：

作者认为边缘设备使用的的延迟由两部分组成：一部分是模型加载延迟，一部分是推理计算延迟，如下图：模型加载延迟占了90%，减少模型加载延迟可以显著减少边缘设备总延迟。

![img](https://s3.bmp.ovh/imgs/2025/04/22/561919fee33f1a2a.png)

**问题1——什么是模型加载延迟？**

在边缘计算场景中，**模型加载延迟（Model Loading Latency）** 主要指将AI模型从存储设备（如硬盘、内存）**加载到GPU/TPU等计算设备显存**中的时间。

由于模型加载延迟是指将模型参数加载到显存(内存)的时间，如果模型的一部分权重(参数)已经在显存中，那么就不需要换入换出，就可以显著减少模型加载延迟。因此作者想到了用参数共享的方法，留下一部分参数不换出，直接留给下个模型使用。

**问题2——参数共享发生在哪些模型之间？(什么时候可以进行参数共享)**

本文的**参数共享（Parameter Sharing）** 主要发生在**同一边缘服务器上部署的不同下游模型之间**

1. 同一预训练模型衍生的下游模型

   ：

   - 例如，基于ResNet-50预训练模型，通过**层冻结（Layer Freezing）** 或**参数高效微调（如LoRA [38]）** 得到多个任务专用模型（如Task 1分类猫狗，Task 2分类车辆）。
   - 这些下游模型的**底层参数（如卷积层）** 直接共享预训练模型的权重（如图2所示）。

2. 结构相似的异构模型

   ：

   - 不同架构的模型可能部分层功能相似（如CNN的浅层特征提取层），可通过参数对齐实现共享。

**作者通过设计使用PartialLoading来最大化参数共享，进而降低延迟，最大化模型吞吐量**

问题3——**PartialLoading如何结合用户调度与带宽分配以最大化吞吐量？**

- 用户调度：通过DP算法或者贪心算法确定用户调度最优顺序。

  ### 具体应用

  ### **动态规划（DP）算法的应用**

  ### **适用场景**

  针对**“底层共享”特例**（即模型仅在底层共享参数，如图2中的ResNet微调模型）：

  - **问题特征**：共享参数集中在模型底部，调度顺序的影响可被结构化分解。

  - DP设计思路

    ：

    1. **状态定义**：以“已加载的共享层”和“剩余待调度用户”为状态；
    2. **状态转移**：每次选择一个用户，其模型与当前已加载模型的共享层数最大化；
    3. **最优子结构**：全局最优解可通过子问题（部分用户的调度）的最优解递推得到。

  - **复杂度**：多项式时间（如O($n^2$)），适用于边缘服务器实时计算。

  ### **示例**

  假设有3个任务（模型A、B、C），共享底层参数块：

  - DP会优先调度共享层数多的模型（如A→B→C），避免重复加载共享部分。

  ------

  ### **贪心算法的应用**

  ### **适用场景**

  针对**通用情况**（共享参数块出现在模型的任意层，如CNN中层或LLM的注意力模块）：

  - **问题特征**：参数共享位置无规律，DP的状态空间爆炸（无法高效分解）。
  - 贪心策略
    1. **每一步局部最优**：每次选择与当前已加载模型共享参数最多的待加载模型；
    2. **启发式规则**：优先复用最大可能的参数块，减少即时加载时间。
  - **优势**：计算高效（如$O(nlog⁡n)$），适合动态边缘环境；
  - **代价**：可能得到次优解，但实验证明其实际性能接近最优。

  ### **示例**

  若模型A与当前加载模型共享50%参数，模型B共享30%，则优先调度A。

  ### **示例**

  假设边缘服务器需处理3个任务：

  - **模型A**（Task 1）：共享底层参数块α；
  - **模型B**（Task 2）：共享α + 中层参数块β；
  - **模型C**（Task 3）：无共享参数。

  **传统调度**：随机顺序（如A→C→B）→ 每次加载全量参数，总加载时间 = T(A) + T(C) + T(B)。 **PartialLoading调度**：A→B→C → 加载B时仅需加载非共享部分（β），节省α的加载时间。

- **带宽分配问题**

  - **目标**：在调度顺序确定后，为每个用户分配频谱带宽，确保其数据传输与模型加载时间匹配。存在**闭式最优解**（可通过凸优化直接求解）；

三、面临的挑战和下一步的计划

1、论文阅读量少：需要对本周的四篇论文进一步学习，加深理解的同时多阅读其他相关的论文，**对当前研究领域的综述性了解很欠缺**。

2、基础知识学习上慢于预期：没有完成本周结束深度学习的目标，下周还需要3天左右的时间，同时相关知识的薄弱点还亟待巩固加强。

3、还没有展开**[edge-llm-serving](https://github.com/ZinuoCai/edge-llm-serving)**这一项目的动手训练，下周需要在这一项目上有所进展。