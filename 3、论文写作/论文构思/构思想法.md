**预填充解码分离：**

1、量化处理：在预处理内部使用张量并行，解码内部使用流水线并行, 两个阶段使用相同模型但采取不同的实例，实现**内存消耗降低**、**降低延迟**、**提高吞吐量**，同时**提供容错机制**，若Nano宕机，Orin NX自动接管解码（降级模式）。

2、K/V缓存处理：KV缓存在prefill阶段生成后异步传输给Decode阶段设备，这样prefill阶段未结束，Decode阶段设备已经能接收到KV缓存，通过并行KV缓存的传输和prefill的前向推理 将通信时延隐藏在计算时延中，提高了系统的并行度，解决了两阶段分离KV缓存的传输问题。

3、区别调度池：

预填充阶段维护一个调度池，解码阶段维护一个调度池，两个阶段模型分别从对应的调度池进行请求调度。当一个请求到来时，先把它放到预填充调度池，预填充完毕后放入批处理调度池

4、实现迭代级批处理：

PD分离后(Prefill和Decode) 天然把n * d和 1 * d输入区分开了。

因此非常适合做迭代级调度，因为每次的输入仅仅有一个token，可以进行迭代级批处理以加速模型推理。

而预填充阶段由于prompt输入长度可能不一样，其可以实施请求级批处理 ，PD分离后请求级批处理等价于迭代级批处理(但因为只输出一个token TTFT，相当于只迭代一次，也实现了迭代级调度)。

实现迭代级批处理可以有效提高FFN层 GPU的并发度，进而提高系统性能。迭代级批处理的具体实现参考原始论文



**重点可以改进方向一   : 联系紧密但技术获取难，学习曲线陡峭**：

联系紧密的原因：预填充阶段使用一套精度，解码阶段使用一套精度，即将模型编译成两套计算图。由于这两个阶段输入和计算量不同，因此可以**针对这两个阶段分别进行计算图的改进**。例如解码阶段计算图固定输入为1 * d，而预填充阶段输入为n * d。（如果两个阶段不分离的话，解码阶段需要处理的输入也是n*d，但这显然拖慢了处理速度，不如定制 1 * d 的速度更快）。不仅输入维度不一样，在注意力模块(消耗最大计算量) 的计算方式也不一样：预填充阶段是矩阵(Q：n×d) × 矩阵( K的转置：d×m) 以及  注意力权重softmax(Q * T的转置)  （n * m）× V ( m × d)。

而解码阶段的计算量为 1×d mult d×m  和 注意力权重softmax(Q * T的转置)  （1 * m）× V ( m × d)。 从矩阵乘矩阵转为向量乘矩阵，可以从编译，甚至从硬件的角度进行改进和性能提升。(图2)

当前关于深度学习模型编译的资料较少，有以下原因

2、技术获取难点：

**1). 技术栈封闭性**

- **厂商绑定**：计算图优化依赖硬件厂商工具链
  - 计算图技术与硬件加速技术与厂商高绑定：TensorFlow → TFLite；PyTorch → ONNX → TensorRT/TVM
  
    - **专用推理引擎**：
  
    - NVIDIA Jetson → TensorRT
  
    - 高通Hexagon → TFLite with DSP Delegate
  
    - 树莓派 → TFLite + XNNPACK
  
- **黑盒性**：编译器优化（如算子融合）由框架自动完成(Nvidia支持pytorch，google支持TensorFlow)，用户感知弱。

**2). 学术与工业的侧重点不同**

- **学术界**：聚焦模型压缩（蒸馏、量化），易发论文且可复现。
- **工业界**：直接使用厂商优化好的计算图（如TFLite），细节不透明。

**3). 学习曲线陡峭**

- 计算图优化涉及：
  - 编译器原理（如TVM的Relay IR）。
  - 硬件指令集（如ARM NEON、NPU指令）。
  - 跨框架转换（如PyTorch → ONNX → TensorRT）。







**深入方向二**：利用已有技术从模型压缩与模型切分和调度入手

可行方案(利用已有技术)：

1、压缩-------剪枝：对模型进行参数审查一遍，丢弃小于指定阈值的权重

2、在模型的FFN层进一步考虑使用**MoE（专家混合）**



其他方案：

1、压缩------知识蒸馏(模型部署前选取合适的知识蒸馏方法): 相当于用小模型替换大模型，而不是大模型本身的边缘推理部署

2、弹性伸缩——根据负载动态启停设备（如Docker容器化部署）









### **工业级部署 = 压缩 + 计算图优化 + 硬件指令生成 + 模型切分与调度**

以下是这一扩展框架的详细解析，结合最新技术趋势和实际用例：

------

### **一、模型切分（Partitioning）**

#### **1. 切分维度**

| **切分类型**        | **粒度**               | **适用场景**                        | **典型案例**                   |
| :------------------ | :--------------------- | :---------------------------------- | :----------------------------- |
| **模型并行**        | 按层切分（Layer-wise） | 单设备内存不足时（如LLM分层部署）   | Megatron-LM的层间并行          |
| **张量并行**        | 按算子切分（Op-wise）  | 超大矩阵运算（如Attention头的拆分） | ColossalAI的Tensor Parallelism |
| **流水线并行**      | 按数据阶段切分         | 多设备间负载均衡                    | GPipe的微批次流水线            |
| **MoE（专家混合）** | 按条件激活子模型       | 稀疏化计算（如Switch Transformer）  | Google的GLaM模型               |

#### **2. 边缘设备协同中的切分应用**

- **预填充-解码分离**：
  - 将LLM的预填充（高计算量）和解码（低计算量）阶段拆分到不同设备（如Jetson AGX Orin + Jetson Nano），如您之前的构想。
- **KV Cache分片存储**：
  - 在多设备间分布式存储KV Cache，减少单设备内存压力（类似DeepSpeed-Inference的KV Cache分区）。

------

### **二、调度（Scheduling）**

#### **1. 核心目标**

- **资源利用率最大化**：动态分配计算任务到空闲设备。
- **延迟与吞吐平衡**：通过批处理（Batching）和优先级队列优化响应时间。

#### **2. 关键技术**

| **技术**       | **作用**                                   | **边缘设备案例**                   |
| :------------- | :----------------------------------------- | :--------------------------------- |
| **动态批处理** | 合并多个请求的输入，提高GPU/NPU利用率      | NVIDIA Triton的Inference Server    |
| **请求插空**   | 在长序列推理间隙插入短请求，减少等待时间   | vLLM的Continuous Batching          |
| **参数共享**   | 多任务共用同一模型实例，减少内存占用       | TensorFlow Serving的模型多版本加载 |
| **弹性伸缩**   | 根据负载动态启停设备（如Docker容器化部署） | Kubernetes + TensorRT-LLM          |

#### **3. 边缘场景的特殊挑战**

- **设备异构性**：需调度器感知不同设备的算力（如Orin NX的GPU vs Nano的CPU）。
- **网络延迟**：跨设备通信（如KV Cache同步）需低延迟协议（如RDMA或ZeroMQ）。

------

### **三、完整技术栈示例（以LLM边缘部署为例）**

#### **1. 模型准备阶段**

- **压缩**：
  - 量化：FP32 → INT8（针对Attention矩阵）
  - 剪枝：移除10%的注意力头
- **切分**：
  - 按层切分模型，前N层部署到Orin NX，剩余层部署到Nano。

#### **2. 计算图优化**

- **导出为ONNX**：

  python

  

  复制

  

  下载

  ```
  torch.onnx.export(
      model,
      inputs,
      "llm.onnx",
      opset_version=18,
      input_names=["input_ids", "past_kv"],
      dynamic_axes={
          "input_ids": {0: "batch", 1: "seq_len"},
          "past_kv": {3: "history_len"}
      }
  )
  ```

- **编译优化**：

  - Orin NX端：TensorRT生成FP16引擎（`trtexec --onnx=llm.onnx --fp16`）。
  - Nano端：TVM编译为INT8代码（`tvmc compile --target "llvm -mcpu=cortex-a72"`）。

#### **3. 切分与调度实现**

- **设备间分工**：

  python

  

  复制

  

  下载

  ```
  # Orin NX（预填充设备）
  def prefill(input_ids):
      outputs = trt_engine.run(input_ids)
      send_kv_cache_to_nano(outputs.past_kv)  # 通过ZeroMQ发送
  
  # Nano（解码设备）
  while True:
      kv_cache = receive_kv_cache()
      next_token = decode_step(new_token, kv_cache)
      send_token_to_user(next_token)
  ```

- **动态批处理**：

  - 在Orin NX上合并多个用户的输入序列，一次性生成所有KV Cache。

#### **4. 性能监控与弹性调度**

- **资源感知**：
  - 监控设备负载（如GPU利用率），动态分配任务。
- **容错机制**：
  - 若Nano宕机，Orin NX自动接管解码（降级模式）。

------

### **四、为什么需要这一扩展？**

#### **1. 大模型的新需求**

- **内存墙**：单设备无法容纳LLM全部参数（如LLaMA-7B仅权重需14GB FP16）。
- **实时性要求**：边缘场景需低延迟响应（如车载语音助手）。

#### **2. 硬件多样性**

- **异构计算**：需协调GPU、NPU、CPU等不同计算单元。

#### **3. 成本控制**

- **资源复用**：通过调度实现多租户共享设备，降低部署成本。

------

### **五、挑战与解决方案**

| **挑战**             | **解决方案**                                       |
| :------------------- | :------------------------------------------------- |
| **切分后的通信开销** | 使用RDMA或压缩传输（如FP16 → INT8量化KV Cache）    |
| **调度延迟**         | 预生成调度策略（如离线分析模型计算图，规划切分点） |
| **设备异构编程**     | 统一抽象层（如Apache TVM的Universal IR）           |

------

### **六、学习路径建议**

1. **基础**：
   - 掌握模型并行框架（如Megatron-LM、ColossalAI）。
   - 学习调度器设计（如Kubernetes + Triton Inference Server）。
2. **进阶**：
   - 研究MoE架构（如Switch Transformer）的边缘部署。
   - 探索编译器与调度器的协同优化（如TVM + Ray）。
3. **实战**：
   - 在Jetson集群上部署多设备协同的LLM服务（参考[NVIDIA TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)）。

------

### **总结**

您的洞察完全正确——**模型切分与调度**已成为LLM边缘部署的核心支柱。完整的工业级流程应是：
**压缩 → 计算图优化 → 切分 → 硬件指令生成 → 动态调度**
这一框架能平衡**效率、资源、延迟**，是支撑ChatGPT等应用落地的关键。