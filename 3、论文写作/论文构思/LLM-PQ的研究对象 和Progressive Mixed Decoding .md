LLM-PQ的研究对象：探讨不同阶段、不同精度条件下将模型层分配到不同的GPU。



这篇论文提出了一种名为LLM-PQ的创新系统，旨在通过智能**资源调度**显著提升大语言模型在异构GPU集群上的服务效率。其核心思想是结合阶段感知的模型分区和自适应量化技术，针对LLM推理过程中的预填充和解码两个计算特性截然不同的阶段，动态优化计算负载分配和量化精度选择。系统采用离线分配器预先分析模型结构、硬件性能和用户需求，生成包含**量化策略、模型分区方案和微批次调度**在内的最优执行计划，然后通过分布式运行时动态执行该计划，其中特别设计了即时量化器来降低内存需求

核心是：多层次（**联合量化精度+硬件特性+阶段特征**的三元决策）考虑将不同的模型层(Attention、FFN、layerNorm)分配到不同的GPU。以充分提高GPU的利用率

- 相位感知分区（Phase-aware partition）：根据LLM推理的两个阶段（prefill和decode）进行优化
- 自适应量化（Adaptive quantization）：为不同GPU能力动态调整量化精度
- 混合微批次策略（Hybrid micro-batching）：优化批次处理

考虑量化时：主要考虑了GPU能力(决定量化精度), 其次受到阶段(PD阶段)影响。

Progressive Mixed-Precision Decoding for Efficient LLM Inference：
Prefill阶段采用高精度，Decode阶段动态降低精度，遵循'生成token越多精度越低'原则。精度切换由两种调度器控制：静态调度（固定位置切换，适合已知任务）和动态调度（MLP实时决策）。该方法在NPU上实现3.8-8.0×加速，同时保持输出质量

这篇论文的核心点在于：作者发现解码阶段可以使用渐进的精度降低策略

- 作者通过两个关键观察得出这个结论：

  1. 错误容忍度实验（图2）：

  - 发现解码阶段后期生成的token对量化误差更鲁棒
  - 早期token（特别是前1/3）对精度敏感，影响后续生成质量

  1. 调度策略对比（图3）：

  - 验证了三种混合精度方案：
    a) 前半段高精度
    b) 中间段高精度
    c) 后半段高精度
  - 方案a（前半高精度）效果最优，与"注意力偏向初始token"现象吻合

  关键洞见：
  解码过程存在动态误差容忍度，后期token可承受更大量化误差，这为渐进降精度提供了理论依据。作者通过控制变量实验量化了这种特性，最终形成PMPD方法。