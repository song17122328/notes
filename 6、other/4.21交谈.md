## 一、近期状态

#### 1、个人状态

早8晚10的实验室节奏，下午4点50-5点40会去南区体育场跑个3km-5km，一是减肥锻炼身体，二是提高学习的效率，专注度和精力。周末没有约的时候都在是实验室，我觉得在实验室还挺舒服的。

#### 2、学习状态和体会到的一点科研状态

training阶段的状态：需要学到东西很多，但还是很开心的，每天都能学到新的内容。

最近在专攻李沐的《动手学深度学习》，这周把RNN结束掉了，下周应该就能完整整本书的学习。

关于难度和压力：一个是学的东西会开头20%难一点，花的时间多一点，开头攻克了后面浏览一遍的速度很快，知识也记的七七八八了，整体上没什么压力，都能接受。关于考核，training也没有考试，不像考研需要刷题，任务就每周只需要写一个周报，只需要思考和写作，我也挺喜欢写作和思考，周报也没什么压力。

在学习的过程中，我能逐渐体会到拿着一把锤子找钉子的感觉，比如上周出差我在完成前后端，中间件对接后，西电那边还要求给出能运行的算法，当时我跟804的黄宇轩给出了一个demo示例算法占了位置，但我自己想过如何解决这个问题，给了既定的输入和输出，可以拿神经网络去学习和拟合其中的概率分布。越来越理解到神经网络作为一门语言可以对现实问题进行建模。

## 二、当前的对待科研的方法论：学习思路和学习策略

#### 

**1、对待知识的态度**：学神经网络是在刷课，李沐讲的确实很细，每节视频里面的QA我觉得是非常有用的东西，会把他自己对神经网络的理解，和他自己在工程实践中的经验讲出来。基本看视频就够了，碰到解释不清楚的地方就去看看书，还不清楚再去问一问DeepSeek都能解决掉。

**2、对待论文的态度**

第一遍，直接去问去问大模型，DeepSeek帮我总结一遍，chatGPT帮我总结一便，两者一结合基本很短时间内就能掌握到一篇论文在做什么东西，效率非常高。

第二遍，去弄懂论文提到的关键公式、实验、提出的推理方法、逻辑框架，图表这些东西，同时思考作者为什么会这样做，自己去做有哪些改进空间。



## 三、当前还需要的：拿到新课题做

我的劣势：积累很少，课题组有些同学都有蛮多课题积累的，可能本科就跟着做。

想要老师直接给到一个新课题，我就一边做，一边学习别的知识。单纯只training，没有做到新课题，还是有一种闭门造车没有融进去的感觉，不踏实。我还是非常想拿到一个课题去做，每周参与组会看看他人的进度，有一种融进去的感觉。

- 帮师兄调参打下手
- 复现某篇论文的基础实验
- 或者您觉得哪个方向最急需人手？"











### **二、工作层面：分阶段攻坚**

#### **阶段1（1-3个月）：快速建立知识地图**

- **核心任务**：确定研究方向，补足基础短板。
- **具体行动**：
  1. **与导师深度沟通**：
     - 明确导师的主攻方向（如AI、体系结构、安全）。
     - 请求导师推荐**3-5篇领域奠基性论文**（Survey/Review类优先）。
  2. **技术栈速成**：
     - **编程**：LeetCode中级题（每天1题）+ GitHub开源项目阅读（如PyTorch源码）。
     - **数学**：重点补与研究相关的数学（如做机器学习需精通概率论、线性代数）。
     - **工具**：掌握科研工具链（LaTeX、Git、Linux命令行、实验框架如TensorFlow）。
  3. **文献管理**：
     - 用Zotero/EndNote建立文献库，按“理论基础/方法/应用”分类。

#### **阶段2（3-6个月）：小规模试错产出**

- **核心任务**：参与项目，尝试复现或改进已有工作。
- **具体行动**：
  1. **选择1个纵向课题子任务**（如数据预处理、算法调参）：
     - 目标：理解完整科研流程（问题定义→方法设计→实验→分析）。
  2. **复现1篇顶会论文**：
     - 选择代码开源的论文（如NeurIPS、CVPR），复现后尝试：
       - 更换数据集测试泛化性
       - 调整超参数观察性能变化
     - **输出**：撰写复现报告（含问题记录），这是未来论文的“Related Work”素材。
  3. **参加组会汇报**：
     - 即使不成熟也要主动展示，获取反馈比“准备完美”更重要。

#### **阶段3（6-12个月）：独立研究突破**

- **核心任务**：产出第一篇一作论文。
- **具体行动**：
  1. **从项目中提炼创新点**：
     - 例如：横向项目的实际需求→发现现有算法缺陷→提出改进方案。
  2. **“三明治”写作法**：
     - 模仿顶会论文结构（摘要→引言→方法→实验），但替换为自己的内容。
  3. **主动寻求合作**：
     - 与师兄师姐联合论文（负责实验/可视化），学习写作技巧。

------

### **三、关键策略：高效学习法**

1. **“80/20法则”聚焦重点**
   - 计算机领域知识无限，优先掌握：
     - 研究方向的核心理论（如做NLP需精通Transformer）。
     - 领域内近年顶会的**5篇最高引论文**。
2. **构建“问题树”**
   - 用思维导图整理：
     - 主干问题（如“如何提升模型鲁棒性”）
     - 分支方法（对抗训练、数据增强等）
     - 开放问题（文献中未解决的难点）
3. **每日科研“三板斧”**
   - 1小时读论文（精读摘要+方法，略读实验）。
   - 1小时写代码（实验或算法实现）。
   - 1小时写总结（哪怕只有100字）。





An LLM service responds to a user query in two phases.
The prefill phase processes a user’s prompt, composed of a sequence of tokens, to generate the first token of the response in one step. Following it, the decoding phase sequentially generates subsequent tokens in multiple steps; each decoding step generates a new token based on tokens generated in previous steps, until reaching a termination token. This dualphase process distinguishes LLM services from traditional services – an LLM service’s latency is uniquely measured by two key metrics: the time to first token (TTFT), which is the duration of the prefill phase, and the time per output token (TPOT), which represents the average time taken to
generate a token for each request (except for the first token)1.
Different applications place varying demands on each metric.
For example, real-time chatbots [1] prioritize low TTFT for
response promptness, while TPOT only remains important until
it is faster than human reading speed (i.e., 250 words/min).
Conversely, document summarization emphasizes low TPOT
for faster generation of the summary.

每天要做的工作

P、C、S、K

- 1小时读论文（精读摘要+方法，略读实验）。
- 1小时写代码（实验或算法实现、不需要复现实现的时候，做一道leetcode题目）。
- 1小时学习掌握知识
- 1小时写总结（哪怕只有100字）。
